_target_: src.utils.transformers.FullCrossAttentionEncoder
_partial_: true

use_lite: False
node_embd_config:
  act_h: lrlu
  nrm: layer
ctxt_embd_config:
  outp_dim: 64
  act_h: lrlu
  nrm: layer
cae_config:
  model_dim: 64
  num_layers: 3
  mha_config:
    num_heads: 16
    init_zeros: True
    do_layer_norm: True
  dense_config:
    hddn_dim: 256
    act_h: lrlu
    nrm: layer
    output_init_zeros: True
outp_embd_config:
  act_h: lrlu
  nrm: layer
  output_init_zeros: True
