# @package _global_

# Order indicates overwriting
defaults:
  - _self_
  - trainer: default.yaml
  - model: default.yaml
  - datamodule: default.yaml
  - loggers: default.yaml
  - hydra: default.yaml
  - paths: default.yaml
  - callbacks: default.yaml
  - experiment: null

username: mleigh # For weights and biases login
project_name: new_models # Determines output directory path and wandb project
network_name: ${now:%Y-%m-%d}_${now:%H-%M-%S-%f} # Used for both saving and wandb
ckpt_path: null  # Checkpoint path to resume training
seed: 12345 # For reproducibility

# Extra tweaks available with the pytorch 2.0
precision: highest # Should use medium if on ampere gpus
compile: null # Can set to 'default' for faster inference (doesnt work for now)

# COMPLETELY replaces the above config with what is contained in ${paths.full_path}
# This is ideal for resuming a job, log to the same directory
# Will also resume the loggers and set the ckpt_path to the latest
full_resume: False
